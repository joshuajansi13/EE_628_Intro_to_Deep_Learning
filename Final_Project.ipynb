{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_training_data(name, label, save_folder, num_samples):\n",
    "    '''\n",
    "    Creates the training data set by capturing a certain number of face samples from your video camera.\n",
    "    The face detection model used for this part is the Haar Cascade model.\n",
    "    '''\n",
    "    face_cascade = cv2.CascadeClassifier( cv2.data.haarcascades + \"haarcascade_frontalface_default.xml\" )\n",
    "\n",
    "    # Initialize and start realtime video capture\n",
    "    cam = cv2.VideoCapture(0)\n",
    "    cam.set(3, 640) # set video width\n",
    "    cam.set(4, 480) # set video height\n",
    "    \n",
    "    count = 0\n",
    "\n",
    "    while True:\n",
    "        ret, img = cam.read()\n",
    "\n",
    "        if ret == True:\n",
    "            gray_scale = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "            faces = face_cascade.detectMultiScale(gray_scale, 1.2, 4, minSize=(30, 30))\n",
    "\n",
    "            for (x, y, w, h) in faces:\n",
    "                cv2.rectangle(img, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
    "                count += 1\n",
    "\n",
    "                # Save the image \n",
    "                cv2.imwrite(save_folder + \"/\" + label + \"_\" + name + \"_\" + str(count) + \".jpg\", gray_scale[y:y+h, x:x+w])\n",
    "                cv2.imshow('Create Training Data', img)\n",
    "        \n",
    "        k = cv2.waitKey(100) & 0xff # Press 'ESC' for exiting video\n",
    "        if k == 27:\n",
    "            break\n",
    "        elif count >= int(num_samples): # Take num_samples face samples and stop video\n",
    "            break\n",
    "\n",
    "    cam.release()\n",
    "    cv2.destroyAllWindows()\n",
    "    print(\"Finished collecting samples.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = input(\"Please enter the name of the person being recorded: \")\n",
    "label = input(\"Please enter a label for this person: \")\n",
    "num_samples = input(\"Please enter the number of samples you would like to collect: \")\n",
    "\n",
    "create_training_data(name=name, label=label, save_folder=\"training_data\", num_samples=num_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(data_folder):\n",
    "    data = os.listdir(data_folder)\n",
    "\n",
    "    names = []\n",
    "    labels = []\n",
    "    faces = []\n",
    "\n",
    "    for img_name in data:\n",
    "        name = img_name.split('_')[1]\n",
    "        label = int(img_name.split('_')[0])\n",
    "\n",
    "        image_path = data_folder + \"/\" + img_name\n",
    "\n",
    "        img = cv2.imread(image_path)\n",
    "\n",
    "        gray_scale = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "        names.append(name)\n",
    "        labels.append(label)\n",
    "        faces.append(cv2.resize(gray_scale, (254, 254)))\n",
    "\n",
    "    return names, faces, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Loading training data...\")\n",
    "names, faces_raw, labels = load_data(\"training_data\")\n",
    "names = list(dict.fromkeys(names))\n",
    "faces = np.array(faces_raw) / 255\n",
    "print(\"Data prepared\")\n",
    "\n",
    "print(f\"Number of names: {len(names)}\")\n",
    "print(f\"Number of faces: {len(faces)}\")\n",
    "print(f\"Number of labels: {len(labels)}\")\n",
    "\n",
    "number_of_classes = len(names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LBPH Face Recognizer model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_lbph_recognizer(faces, labels):\n",
    "    recognizer = cv2.face.LBPHFaceRecognizer_create()\n",
    "    recognizer.train(faces, np.array(labels))\n",
    "\n",
    "    return recognizer\n",
    "\n",
    "# Train the face recognizer model and save the model\n",
    "print(\"Training LBPH Face Recognizer model...\")\n",
    "lbph_model = train_lbph_recognizer(faces, labels)\n",
    "print(\"Done training\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VGG16 model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Conv2D, MaxPool2D , Flatten\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "vgg_model = Sequential()\n",
    "vgg_model.add(Conv2D(filters=64,kernel_size=(3,3),padding=\"same\", activation=\"relu\", input_shape=(254,254,1)))\n",
    "vgg_model.add(Conv2D(filters=64,kernel_size=(3,3),padding=\"same\", activation=\"relu\"))\n",
    "vgg_model.add(MaxPool2D(pool_size=(2,2),strides=(2,2)))\n",
    "vgg_model.add(Conv2D(filters=128, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n",
    "vgg_model.add(Conv2D(filters=128, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n",
    "vgg_model.add(MaxPool2D(pool_size=(2,2),strides=(2,2)))\n",
    "vgg_model.add(Conv2D(filters=256, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n",
    "vgg_model.add(Conv2D(filters=256, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n",
    "vgg_model.add(Conv2D(filters=256, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n",
    "vgg_model.add(MaxPool2D(pool_size=(2,2),strides=(2,2)))\n",
    "vgg_model.add(Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n",
    "vgg_model.add(Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n",
    "vgg_model.add(Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n",
    "vgg_model.add(MaxPool2D(pool_size=(2,2),strides=(2,2)))\n",
    "vgg_model.add(Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n",
    "vgg_model.add(Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n",
    "vgg_model.add(Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n",
    "vgg_model.add(MaxPool2D(pool_size=(2,2),strides=(2,2)))\n",
    "vgg_model.add(Flatten())\n",
    "vgg_model.add(Dense(units=4096,activation=\"relu\"))\n",
    "vgg_model.add(Dense(units=4096,activation=\"relu\"))\n",
    "vgg_model.add(Dense(units=number_of_classes, activation=\"softmax\"))\n",
    "\n",
    "vgg_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "vgg_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import to_categorical\n",
    "\n",
    "y_data = to_categorical(np.array(labels), number_of_classes)\n",
    "train_data = faces.reshape(np.array(faces).shape[0], 254, 254, 1)\n",
    "\n",
    "vgg_model.fit(train_data, y_data, epochs=15, sbatch_size=32)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AlexNet model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Conv2D, MaxPool2D , Flatten, Dropout\n",
    "\n",
    "alexnet_model = Sequential()\n",
    "\n",
    "# Layer 1\n",
    "alexnet_model.add(Conv2D(96, (11, 11), strides=(4, 4), activation='relu', input_shape=(254, 254, 1)))\n",
    "alexnet_model.add(MaxPool2D((3, 3), strides=(2, 2)))\n",
    "\n",
    "# Layer 2\n",
    "alexnet_model.add(Conv2D(256, (5, 5), padding='same', activation='relu'))\n",
    "alexnet_model.add(MaxPool2D((3, 3), strides=(2, 2)))\n",
    "\n",
    "# Layer 3\n",
    "alexnet_model.add(Conv2D(384, (3, 3), padding='same', activation='relu'))\n",
    "\n",
    "# Layer 4\n",
    "alexnet_model.add(Conv2D(384, (3, 3), padding='same', activation='relu'))\n",
    "\n",
    "# Layer 5\n",
    "alexnet_model.add(Conv2D(256, (3, 3), padding='same', activation='relu'))\n",
    "\n",
    "# Flatten before dense layers\n",
    "alexnet_model.add(Flatten())\n",
    "\n",
    "# Dense layers\n",
    "alexnet_model.add(Dense(4096, activation='relu'))\n",
    "alexnet_model.add(Dropout(0.5))\n",
    "alexnet_model.add(Dense(4096, activation='relu'))\n",
    "alexnet_model.add(Dropout(0.5))\n",
    "alexnet_model.add(Dense(number_of_classes, activation='softmax'))\n",
    "\n",
    "alexnet_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "alexnet_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import to_categorical\n",
    "\n",
    "y_data = to_categorical(np.array(labels), number_of_classes)\n",
    "train_data = faces.reshape(np.array(faces).shape[0], 254, 254, 1)\n",
    "\n",
    "alexnet_model.fit(train_data, y_data, epochs=15, batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test the different models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(model, model_name, names):\n",
    "    face_cascade = cv2.CascadeClassifier( cv2.data.haarcascades + \"haarcascade_frontalface_default.xml\" )\n",
    "    font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "\n",
    "    # Initialize and start realtime video capture\n",
    "    cam = cv2.VideoCapture(0)\n",
    "    cam.set(3, 640) # set video width\n",
    "    cam.set(4, 480) # set video height\n",
    "\n",
    "    while True:\n",
    "        ret, img = cam.read()\n",
    "        gray_scale = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "        faces = face_cascade.detectMultiScale(gray_scale, scaleFactor=1.2, minNeighbors=4, minSize=(30,30))\n",
    "\n",
    "        for (x, y, w, h) in faces:\n",
    "            cv2.rectangle(img, (x, y), (x+w, y+h), (0, 255, 0), 2)\n",
    "            face = gray_scale[y:y+h, x:x+w]\n",
    "            face_resized = np.array(cv2.resize(face, (254, 254))) / 255\n",
    "\n",
    "\n",
    "            if model_name == \"lbph\":\n",
    "                id, confidence = model.predict(face_resized)\n",
    "\n",
    "                if (confidence < 100):\n",
    "                    name = names[id]\n",
    "                    confidence = \"  {0}%\".format(round(100 - confidence))\n",
    "                else:\n",
    "                    name = \"unknown\"\n",
    "                    confidence = \"  {0}%\".format(round(100 - confidence))\n",
    "            else:\n",
    "                confidence = \"\"\n",
    "                face_resized = np.reshape(face_resized, (1, 254, 254, 1))\n",
    "                id = int(np.argmax(model.predict(face_resized), axis=-1))\n",
    "                name = names[id]\n",
    "\n",
    "            cv2.putText(\n",
    "                        img,\n",
    "                        name,\n",
    "                        (x+5, y-5),\n",
    "                        font,\n",
    "                        1,\n",
    "                        (255, 255, 255),\n",
    "                        2\n",
    "                       )\n",
    "            \n",
    "            cv2.putText(\n",
    "                        img,    \n",
    "                        str(confidence), \n",
    "                        (x+5,y+h-5), \n",
    "                        font, \n",
    "                        1, \n",
    "                        (255,255,0), \n",
    "                        1\n",
    "                       ) \n",
    "        \n",
    "        cv2.imshow('Face Recognition', img)\n",
    "        k = cv2.waitKey(10) & 0xff # Press 'ESC' for exiting video\n",
    "        if k == 27:\n",
    "            break\n",
    "\n",
    "    # Do a bit of cleanup\n",
    "    print(\"\\n [INFO] Exiting Program and cleanup stuff\")\n",
    "    cam.release()\n",
    "    cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LBPH model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_model(model=lbph_model, model_name=\"lbph\", names=names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VGG16 model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_model(model=vgg_model, model_name=\"vgg\", names=names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AlexNet model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_model(model=alexnet_model, model_name=\"alexnet\", names=names)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
